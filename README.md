# FAQ Project - Persian RAG System Benchmark

A comprehensive **Retrieval-Augmented Generation (RAG)** system for Persian language question-answering tasks. This project benchmarks different embedding models and vector stores for retrieval performance and evaluates generation quality using BLEU and ROUGE metrics.

## ðŸ“‹ Table of Contents

- [Overview](#overview)
- [Dataset](#dataset)
- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Evaluation Metrics](#evaluation-metrics)
- [Results](#results)
- [Configuration](#configuration)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## ðŸŽ¯ Overview

This project implements a complete RAG pipeline for Persian language FAQ systems with:
- **3 Persian embedding models** for semantic search
- **3 FAISS vector stores** for efficient retrieval
- **LLM-based generation** using OpenAI's GPT-4o
- **Comprehensive evaluation** of both retrieval and generation components
- **Rate limiting** to handle API constraints gracefully

## ðŸ“Š Dataset

This project uses the **Synthetic Persian Chatbot RAG Topics Retrieval** dataset from MCINext:

ðŸ”— **Dataset Link:** [https://huggingface.co/datasets/MCINext/synthetic-persian-chatbot-rag-topics-retrieval](https://huggingface.co/datasets/MCINext/synthetic-persian-chatbot-rag-topics-retrieval)

### Dataset Details:
- **Language:** Persian (Farsi)
- **Type:** Synthetic FAQ dataset for RAG tasks
- **Format:** CSV
- **Contains:**
  - `corpus.csv` - Knowledge base documents with IDs and text
  - `test_enriched.csv` - Test queries with ground truth corpus IDs
- **Use Case:** Training and evaluating Persian language question-answering systems

### How to Use the Dataset:
1. Download from HuggingFace: [MCINext/synthetic-persian-chatbot-rag-topics-retrieval](https://huggingface.co/datasets/MCINext/synthetic-persian-chatbot-rag-topics-retrieval)
2. Extract `corpus.csv` and `test_enriched.csv`
3. Place them in the project root directory
4. Run `data_preparation.py` to process the data

### Citation:
```bibtex
@dataset{mcinext_persian_rag,
  title={Synthetic Persian Chatbot RAG Topics Retrieval},
  author={MCINext},
  year={2024},
  url={https://huggingface.co/datasets/MCINext/synthetic-persian-chatbot-rag-topics-retrieval}
}






Input Query
    â†“
[Embedding Model] â†’ [Vector Store (FAISS)]
    â†“
[Retriever] â†’ Top-K Documents (k=3)
    â†“
[Context + Query] â†’ [LLM (GPT-4o)]
    â†“
Generated Answer
    â†“
[BLEU & ROUGE Evaluation]


git clone https://github.com/AmirMohammadKomijani/FAQ_Porject_RAG.git
cd FAQ_Porject_RAG


# Using venv
python -m venv venv

# Activate virtual environment
# On Linux/Mac:
source venv/bin/activate

# On Windows:
venv\Scripts\activate


pip install -r requirements.txt

# Create .env file in project root
echo "OPENAI_API_KEY=your_api_key_here" > .env

pip install faiss-gpu

FAQ_Porject_RAG/
â”œâ”€â”€ data_preparation.py           # Data loading and Document conversion
â”‚   â”œâ”€â”€ Load corpus.csv
â”‚   â”œâ”€â”€ Load test_enriched.csv
â”‚   â””â”€â”€ Convert to LangChain Document objects
â”‚
â”œâ”€â”€ embedding_models.py           # 3 Persian embedding models setup
â”‚   â”œâ”€â”€ HooshvareLab/bert-fa-base-uncased
â”‚   â”œâ”€â”€ HooshvareLab/sentence-bert-fa-base-stsb
â”‚   â””â”€â”€ HooshvareLab/bert-fa-zeroshot-clf-base
â”‚
â”œâ”€â”€ vector_stores.py              # FAISS vector store creation
â”‚   â”œâ”€â”€ FAISS with model 1
â”‚   â”œâ”€â”€ FAISS with model 2
â”‚   â”œâ”€â”€ FAISS with model 3
â”‚   â””â”€â”€ Create retrievers
â”‚
â”œâ”€â”€ generation.py                 # RAG chain setup with LLM
â”‚   â”œâ”€â”€ Define Persian prompt template
â”‚   â”œâ”€â”€ Initialize GPT-4o
â”‚   â”œâ”€â”€ Create QA chain
â”‚   â””â”€â”€ Create RAG chains
â”‚
â”œâ”€â”€ evaluate_retriever.py         # Retriever evaluation metrics
â”‚   â”œâ”€â”€ Precision calculation
â”‚   â”œâ”€â”€ Recall calculation
â”‚   â”œâ”€â”€ F1-Score calculation
â”‚   â””â”€â”€ Detailed output per query
â”‚
â”œâ”€â”€ evaluate_generator.py         # Generator evaluation metrics
â”‚   â”œâ”€â”€ BLEU score calculation
â”‚   â”œâ”€â”€ ROUGE score calculation
â”‚   â”œâ”€â”€ Rate limiting with delays
â”‚   â””â”€â”€ Break after N requests
â”‚
â”œâ”€â”€ main.py                       # Main execution script
â”‚   â””â”€â”€ Orchestrates all components
â”‚
â”œâ”€â”€ corpus.csv                    # Knowledge base documents
â”‚   â”œâ”€â”€ _id: Document ID
â”‚   â”œâ”€â”€ text: Document content
â”‚   â””â”€â”€ title: Document title
â”‚
â”œâ”€â”€ test_enriched.csv             # Test queries dataset
â”‚   â”œâ”€â”€ query_text: User question
â”‚   â”œâ”€â”€ corpus-id: Ground truth document ID
â”‚   â””â”€â”€ answer: Expected answer
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env                          # Environment variables (not in repo)
â”œâ”€â”€ .gitignore                    # Git ignore rules
â””â”€â”€ README.md                     # This file


python main.py
